{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-eej689Ond4",
    "outputId": "08e8efa9-5c02-4390-a479-9eb893672102"
   },
   "outputs": [],
   "source": [
    "# Perform git clone as follows\n",
    "\n",
    "!git clone TODO\n",
    "!cp -r cse579_HW2/* .\n",
    "\n",
    "# !NOTE!: Once you are done, copy your implementation of policy gradient, actor critic and\n",
    "# in the notebook here back to the python script\n",
    "# when submiting your code\n",
    "\n",
    "!apt-get install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf\n",
    "!pip install setuptools wheel\n",
    "!pip install gym==0.26.2\n",
    "!pip install gymnasium\n",
    "!pip install gymnasium-robotics[mujoco-py]\n",
    "!pip install gym-notices==0.0.8\n",
    "!pip install matplotlib\n",
    "!pip install mujoco\n",
    "!pip install free-mujoco-py\n",
    "!pip install pybullet\n",
    "!pip install tqdm\n",
    "!pip install imageio[ffmpeg] matplotlib\n",
    "import os\n",
    "os.environ['LD_PRELOAD']=':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T02:39:58.355904Z",
     "start_time": "2024-10-24T02:39:57.651553Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import gymnasium as gym\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from utils import log_density, ReplayBuffer\n",
    "from networks import PGPolicy, PGBaseline\n",
    "import random\n",
    "from rollouts import evaluate, evaluate_agent, rollout, rollout_frames\n",
    "import sac\n",
    "from agents import GenericACAgent, train_agent\n",
    "import imageio\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T02:40:03.363416Z",
     "start_time": "2024-10-24T02:40:03.336821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdd\n"
     ]
    }
   ],
   "source": [
    "print(\"sdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(policy, baseline, trajs, policy_optim, baseline_optim, device, gamma=0.99, baseline_train_batch_size=64,\n",
    "                baseline_num_epochs=5):\n",
    "    states_all = []\n",
    "    actions_all = []\n",
    "    returns_all = []\n",
    "    for traj in trajs:\n",
    "        states_singletraj = traj['observations']\n",
    "        actions_singletraj = traj['actions']\n",
    "        rewards_singletraj = traj['rewards']\n",
    "        returns_singletraj = np.zeros_like(rewards_singletraj)\n",
    "        running_returns = 0\n",
    "        for t in reversed(range(0, len(rewards_singletraj))):\n",
    "            running_returns = rewards_singletraj[t] + gamma * running_returns\n",
    "            returns_singletraj[t] = running_returns\n",
    "        states_all.append(states_singletraj)\n",
    "        actions_all.append(actions_singletraj)\n",
    "        returns_all.append(returns_singletraj)\n",
    "    states = np.concatenate(states_all)\n",
    "    actions = np.concatenate(actions_all)\n",
    "    returns = np.concatenate(returns_all)\n",
    "\n",
    "    # Normalize the returns\n",
    "    returns = (returns - returns.mean()) / returns.std() + 1e-8\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    n = len(states)\n",
    "    arr = np.arange(n)\n",
    "    for epoch in range(baseline_num_epochs):\n",
    "        np.random.shuffle(arr)\n",
    "        for i in range(n // baseline_train_batch_size):\n",
    "            batch_index = arr[baseline_train_batch_size * i: baseline_train_batch_size * (i + 1)]\n",
    "            batch_index = torch.LongTensor(batch_index).to(device)\n",
    "            inputs = torch.Tensor(states).to(device)[batch_index]\n",
    "            target = torch.Tensor(returns).to(device)[batch_index]\n",
    "            \n",
    "            #========== TODO: start ==========\n",
    "            # Train baseline by regressing onto returns.\n",
    "            # Hint: Regress the baseline from each state onto the above\n",
    "            # computed return to go. You can use similar code to behavior cloning to do so. This should be\n",
    "            # 2 lines of code\n",
    "\n",
    "\n",
    "\n",
    "            #========== TODO: END ==========\n",
    "            baseline_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            baseline_optim.step()\n",
    "\n",
    "    action, std, logstd = policy(torch.Tensor(states).to(device))\n",
    "    log_policy = log_density(torch.Tensor(actions).to(device), policy.mu, std, logstd)\n",
    "    baseline_pred = baseline(torch.from_numpy(states).float().to(device))\n",
    "    #========== TODO: start ==========\n",
    "    # Train policy by optimizing surrogate objective: -log prob * (return - baseline)\n",
    "    # Hint: Policy gradient is given by: \\grad log prob(a|s)* (return - baseline)\n",
    "    # Hint: Return is computed above, you can computer log_probs using the log_density function imported.\n",
    "    # Hint: You can predict what the baseline outputs for every state.\n",
    "    # Hint: Then simply compute the surrogate objective by taking the objective as -log prob * (return - baseline)\n",
    "    # Hint: You can then use standard pytorch machinery to take *one* gradient step on the policy\n",
    "\n",
    "\n",
    "    \n",
    "    #========== TODO: END ==========\n",
    "    policy_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    policy_optim.step()\n",
    "\n",
    "    del states, actions, returns, states_all, actions_all, returns_all\n",
    "\n",
    "\n",
    "# Training loop for policy gradient\n",
    "def simulate_policy_pg(env, policy, baseline, num_epochs=200, batch_size=100,\n",
    "                       gamma=0.99, baseline_train_batch_size=64, baseline_num_epochs=5, print_freq=10, device = \"cuda\", render=False):\n",
    "    policy_optim = optim.Adam(policy.parameters())\n",
    "    baseline_optim = optim.Adam(baseline.parameters())\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        sample_trajs = []\n",
    "\n",
    "        # Sampling trajectories\n",
    "        for it in range(batch_size):\n",
    "            sample_traj = rollout(\n",
    "                env,\n",
    "                policy,\n",
    "                render=False)\n",
    "            sample_trajs.append(sample_traj)\n",
    "\n",
    "        # Logging returns occasionally\n",
    "        if iter_num % print_freq == 0:\n",
    "            rewards_np = np.mean(np.asarray([traj['rewards'].sum() for traj in sample_trajs]))\n",
    "            path_length = np.max(np.asarray([traj['rewards'].shape[0] for traj in sample_trajs]))\n",
    "            print(\"Episode: {}, reward: {}, max path length: {}\".format(iter_num, rewards_np, path_length))\n",
    "\n",
    "        # Training model\n",
    "        train_model(policy, baseline, sample_trajs, policy_optim, baseline_optim, device, gamma=gamma,\n",
    "                    baseline_train_batch_size=baseline_train_batch_size, baseline_num_epochs=baseline_num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent(GenericACAgent):\n",
    "    def update_actor(self, obs):\n",
    "        dist = self.actor(obs)\n",
    "        # The .rsample() is used to sample using the reparameterization trick to allow for backpropagation\n",
    "        action = dist.rsample()\n",
    "        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n",
    "        #========== TODO: start ==========\n",
    "        # Implement the actor update\n",
    "        # Compute the Q values of the action using self.critic(obs, action). In this case it is a single instead of\n",
    "        # double Q function so you do not need to take a minimum.\n",
    "        # The policy loss is the mean over the negative Q values i.e we want to maximize the Q values\n",
    "\n",
    "        \n",
    "        \n",
    "        #========== TODO: end ==========\n",
    "        # optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        return actor_loss.item(), 0, 0\n",
    "\n",
    "    def update_critic(self, obs, action, reward, next_obs, not_done_no_max):\n",
    "        #========== TODO: start ==========\n",
    "        # Train the single Q function:\n",
    "        # Hint: Step 1: Compute current Q predictions using the obs and action and self.critic()\n",
    "        # Hint: Step 2: Compute q targets using reward + critic_target * not_done_no_max for next_obs and\n",
    "        # next_actions actions sampled from the current policy. Use torch.no_grad() for this step to disable\n",
    "        # gradient flow to the critic_target and the actor.\n",
    "        # Hint: Step 3: Compute Bellman error as mean squared error between q_predictions and q_targets\n",
    "       \n",
    "\n",
    "\n",
    "        #========== TODO: end ==========\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        return critic_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent(GenericACAgent):\n",
    "    def update_actor(self, obs):        \n",
    "        #========== TODO: start ==========\n",
    "        # Sample actions and the log_prob of the actions from the actor given obs. Hint: This is the same as AC agent.\n",
    "        # Get the two Q values from the double Q function critic and take the minimum value. Then calculate the actor loss which\n",
    "        # is defined by self.alpha * log_prob - actor_Q. Make sure that gradient does not flow through the alpha paramater. \n",
    "\n",
    "        \n",
    "        #========== TODO: end ==========\n",
    "        # optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update the temperature parameter toachieve entropy close to the target entropy\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss = (self.alpha *\n",
    "                      (-log_prob - self.target_entropy).detach())\n",
    "        alpha_loss = alpha_loss.mean()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "        return actor_loss.item(), -log_prob.mean().item(), alpha_loss.item()\n",
    "    \n",
    "    def update_critic(self, obs, action, reward, next_obs, not_done_no_max):\n",
    "        #========== TODO: start ==========\n",
    "        # Train the double Q function:\n",
    "        # Hint step 1: Sample the next_action and log_prob of the next action using the self.actor and the next_obs. Use the code \n",
    "        # below in update_actor as a reference on how to do this\n",
    "        \n",
    "        # Hint step 2: Sample the two target Q values from the critic_target using next_obs and the sampled next_action. \n",
    "        # Take these numbers, and get the target value by taking the min of the 2 q values and then subtracting self.alpha*log_prob\n",
    "        # The target Q is the reward + (not_done_no_max * discount * target_value)\n",
    "        \n",
    "        # Hint step 3:\n",
    "        # Sample the current Q1 and Q2 values of the current state using the critic. The loss is mse(Q1, targetQ) + mse(Q2 + target Q)\n",
    "\n",
    "        \n",
    "        #========== TODO: end ==========\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        return critic_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T02:21:27.659218Z",
     "start_time": "2024-10-24T02:21:27.656085Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, task, env, test=False, render=False):\n",
    "        self.task = task # 'pg' or 'actor_critic' or 'sac'\n",
    "        self.env = env # pendulum or ant\n",
    "        self.test = test # whether test only\n",
    "        self.render = render # whether to render during test\n",
    "args = Args('pg', \"pendulum\", False,  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device', device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "if __name__ == '__main__':\n",
    "    if args.env == 'pendulum':\n",
    "        env = gym.make(\"InvertedPendulum-v5\", render_mode='human' if args.render else None)\n",
    "        max_episode_steps = 200\n",
    "        env = gym.wrappers.TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    elif args.env == 'ant':\n",
    "        max_episode_steps = 500\n",
    "        env = gym.make(\"Ant-v5\", render_mode='human' if args.render else None)\n",
    "        env = gym.wrappers.TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    else:\n",
    "        raise ValueError('Invalid environment')\n",
    "\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    ac_size = env.action_space.shape[0]\n",
    "    action_range = [\n",
    "        float(env.action_space.low.min()),\n",
    "        float(env.action_space.high.max())]\n",
    "\n",
    "    if args.task == 'pg':\n",
    "        # Define policy and value function\n",
    "        hidden_dim_pol = 64\n",
    "        hidden_depth_pol = 2\n",
    "        hidden_dim_baseline = 64\n",
    "        hidden_depth_baseline = 2\n",
    "        policy = PGPolicy(obs_size, ac_size, hidden_dim=hidden_dim_pol,\n",
    "                          hidden_depth=hidden_depth_pol)\n",
    "        baseline = PGBaseline(obs_size, hidden_dim=hidden_dim_baseline,\n",
    "                              hidden_depth=hidden_depth_baseline)\n",
    "        policy.to(device)\n",
    "        baseline.to(device)\n",
    "\n",
    "        # Training hyperparameters\n",
    "        num_epochs = 150\n",
    "        batch_size = 64\n",
    "        gamma = 0.99\n",
    "        baseline_train_batch_size = 64\n",
    "        baseline_num_epochs = 5\n",
    "        print_freq = 10\n",
    "\n",
    "        if not args.test:\n",
    "            # Train policy gradient\n",
    "            simulate_policy_pg(env, policy, baseline, num_epochs=num_epochs, batch_size=batch_size,\n",
    "                               gamma=gamma, baseline_train_batch_size=baseline_train_batch_size, device=device,\n",
    "                               baseline_num_epochs=baseline_num_epochs, print_freq=print_freq, render=args.render)\n",
    "            torch.save(policy.state_dict(), 'pg_final.pth')\n",
    "        else:\n",
    "            print('loading pretrained pg')\n",
    "            policy.load_state_dict(torch.load(f'pg_final.pth'))\n",
    "        evaluate(env, policy, num_validation_runs=100, render=args.render)\n",
    "    else:\n",
    "        num_train_steps = 100_000\n",
    "        num_seed_steps = 5000\n",
    "        eval_frequency = 10_000\n",
    "        num_eval_episodes = 10\n",
    "        replay_buffer = ReplayBuffer(obs_size, ac_size, num_train_steps, device)\n",
    "        hidden_dim = 256\n",
    "        hidden_depth = 2\n",
    "        batch_size = 256\n",
    "        discount_factor = 0.99\n",
    "        if args.task == 'actor_critic':\n",
    "            agent = ActorCriticAgent(\n",
    "                obs_dim=obs_size,\n",
    "                action_dim=ac_size,\n",
    "                action_range=action_range,\n",
    "                device=device,\n",
    "                discount=discount_factor,\n",
    "                actor_lr=3e-4,\n",
    "                critic_lr=3e-4,\n",
    "                critic_tau=5e-3,\n",
    "                batch_size=batch_size,\n",
    "                hidden_dim=hidden_dim,\n",
    "                hidden_depth=hidden_depth,\n",
    "                double_critic=False,\n",
    "                temperature=False\n",
    "            )\n",
    "\n",
    "        elif args.task == \"sac\":\n",
    "            agent = sac.SACAgent(\n",
    "                obs_dim=obs_size,\n",
    "                action_dim=ac_size,\n",
    "                action_range=action_range,\n",
    "                device=device,\n",
    "                discount=discount_factor,\n",
    "                init_temperature=0.1,\n",
    "                alpha_lr=3e-4,\n",
    "                actor_lr=3e-4,\n",
    "                critic_lr=3e-4,\n",
    "                critic_tau=0.005,\n",
    "                batch_size=batch_size,\n",
    "                target_entropy=-ac_size,\n",
    "                hidden_dim=hidden_dim,\n",
    "                hidden_depth=hidden_depth,\n",
    "                double_critic=True,\n",
    "                temperature=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Invalid task')\n",
    "\n",
    "        if not args.test:\n",
    "            train_agent(agent,\n",
    "                        env,\n",
    "                        num_train_steps=num_train_steps,\n",
    "                        num_seed_steps=num_seed_steps,\n",
    "                        eval_frequency=eval_frequency,\n",
    "                        num_eval_episodes=num_eval_episodes,\n",
    "                        replay_buffer=replay_buffer)\n",
    "\n",
    "            agent.save(f'{args.task}_final.pth')\n",
    "        else:\n",
    "            print('loading pretrained', args.task)\n",
    "            agent.load(f'{args.task}_final.pth')\n",
    "\n",
    "        # final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.task == \"pg\":\n",
    "    evaluate(env, policy, num_validation_runs=100, render=args.render)\n",
    "else:\n",
    "    evaluate_agent(env, agent, \"final\", num_episodes=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rollouts import rollout_frames\n",
    "if args.env == 'pendulum':\n",
    "    env = gym.make(\"InvertedPendulum-v5\", render_mode='rgb_array')\n",
    "    max_episode_steps = 200\n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "elif args.env == 'ant':\n",
    "    max_episode_steps = 500\n",
    "    env = gym.make(\"Ant-v5\", render_mode='rgb_array')\n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "\n",
    "frames = rollout_frames(env, agent)\n",
    "# show the video, from the list of rgb frames\n",
    "\n",
    "# Assume 'frames' is a list of images (numpy arrays)\n",
    "# Example: frames = [frame1, frame2, frame3, ...]\n",
    "\n",
    "# Define the video file name\n",
    "video_name = 'output_video.mp4'\n",
    "\n",
    "# Create a video writer object using imageio\n",
    "with imageio.get_writer(video_name, fps=25) as writer:\n",
    "    for frame in frames:\n",
    "        # Convert the frame to an image and add it to the video\n",
    "        writer.append_data(frame)\n",
    "\n",
    "# Display the video in the notebook\n",
    "Video(video_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
